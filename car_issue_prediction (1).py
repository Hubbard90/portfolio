# -*- coding: utf-8 -*-
"""Car_Issue_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kX9296lgyQyKE18Owk1VKvtTJIGnHSwP

# Loading
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import shap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import f1_score, classification_report
from xgboost import XGBClassifier

# Load dataset
!git clone https://github.com/Leo-Thomas/EngineFaultDB.git
df = pd.read_csv('/content/EngineFaultDB/EngineFaultDB_Final.csv')
df = df.drop_duplicates()

def add_engine_features(df):
    eps = 1e-8
    df['load_index'] = df['MAP'] * df['TPS'] / (df['RPM'] + eps)
    df['power_per_rpm'] = df['Power']/(df['RPM']+eps)
    df['lambda_dev'] = np.abs(df['Lambda'] - 1)
    df['afr_dev'] = np.abs(df['AFR'] - 14.7)
    df['o2_co2_ratio'] = df['O2']/(df['CO2']+eps)
    df['co2_minus_o2'] = df['CO2'] - df['O2']
    df['combustion_efficiency'] = df['CO2']/(df['CO2']+df['CO']+df['HC']/1e4+eps)
    df['rich_flag'] = (df['Lambda'] < 1).astype(int)
    df['map_x_tps'] = df['MAP']*df['TPS']
    df.replace([np.inf,-np.inf],0,inplace=True)
    df.fillna(0,inplace=True)
    return df

df = add_engine_features(df)

X = df.drop('Fault', axis=1)
y = df['Fault']

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Stratified split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.1, random_state=42, stratify=y
)

# Macro-F1 metric
def macro_f1_eval(y_pred_probs, dtrain):
    y_true = dtrain.get_label()
    y_pred_probs = y_pred_probs.reshape(len(y_true), -1)
    y_pred = np.argmax(y_pred_probs, axis=1)
    return 'macro_f1', f1_score(y_true, y_pred, average='macro')

model = XGBClassifier(
    objective='multi:softprob',
    eval_metric=macro_f1_eval,
    tree_method='hist',
    num_class=len(y.unique()),
    random_state=42,
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

"""# Tuning"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Tuning
param_dist = {
    'n_estimators': randint(200, 900),
    'learning_rate': uniform(0.02, 0.15),
    'max_depth': randint(3, 10),
    'min_child_weight': randint(1, 8),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.5, 0.5),
    'gamma': uniform(0, 0.5),
    'reg_alpha': uniform(0, 1.0),
    'reg_lambda': uniform(1, 10),
}

xgb_tune = XGBClassifier(
    objective='multi:softprob',
    eval_metric=macro_f1_eval,     # <-- uses your correct metric
    tree_method='hist',
    num_class=len(y.unique()),
    random_state=42,
)

# Random search
random_search = RandomizedSearchCV(
    estimator=xgb_tune,
    param_distributions=param_dist,
    n_iter=60,                     # balanced speed + quality
    scoring='f1_macro',            # outer scorer drives selection
    cv=5,
    verbose=2,
    n_jobs=-1,
    random_state=42
)

# Fit
random_search.fit(X_train, y_train)

best_xgb = random_search.best_estimator_

print("\nBest Parameters Found:")
print(random_search.best_params_)

# Evaluation
y_pred = best_xgb.predict(X_test)
print("\nClassification Report (Tuned Model):")
print(classification_report(y_test, y_pred))

"""# Final Model"""

tuned_params = {
    'colsample_bytree': 0.7933755828319241,
    'gamma': 0.482627653632069,
    'learning_rate': 0.1110551371530027,
    'max_depth': 7,
    'min_child_weight': 1,
    'n_estimators': 356,
    'reg_alpha': 0.8021969807540397,
    'reg_lambda': 1.7455064367977082,
    'subsample': 0.9947547746402069
}

new_xgb_model = XGBClassifier(
    objective='multi:softprob',
    eval_metric=macro_f1_eval,
    tree_method='hist',
    num_class=len(y.unique()),
    random_state=42,
    **tuned_params
)

new_xgb_model.fit(X_train, y_train)

y_pred_tuned = new_xgb_model.predict(X_test)

print("\nClassification Report (XGBoost with Tuned Parameters):")
print(classification_report(y_test, y_pred_tuned))

explainer = shap.TreeExplainer(new_xgb_model)
shap_values = explainer.shap_values(X_test)

# Assuming X is a DataFrame and X_test are scaled values corresponding to X
# If X_test is a numpy array, we need feature names for the plot
feature_names = X.columns.tolist()

# For multi-class classification, shap_values will be a list of arrays, one for each class.
# We can plot the mean absolute SHAP values across all classes or for a specific class.
# Let's plot for the first class for now, or consider a global summary.

# To get a single summary plot across all classes, we can take the mean of absolute SHAP values
# across classes. However, shap.summary_plot can handle the list of shap_values directly.

print("Generating SHAP summary plot...")
shap.summary_plot(shap_values, X_test, feature_names=feature_names)

import matplotlib.pyplot as plt

# Assuming shap_values is a 3D array with shape (num_samples, num_features, num_classes)
# and y.unique() gives the class labels [0, 1, 2, 3]

class_labels = y.unique()

# Loop through classes 1, 2, and 3 as requested by the user
for i in range(1, 4):
    # Ensure the class index is within the bounds of the third dimension of shap_values
    if i < shap_values.shape[2]:
        print(f"\nGenerating SHAP summary plot for Class {class_labels[i]}...")
        # Correctly slice the shap_values for the current class
        shap.summary_plot(shap_values[:, :, i], X_test, feature_names=feature_names, show=False)
        plt.title(f'SHAP Summary Plot for Class {class_labels[i]}')
        plt.show()
    else:
        print(f"Warning: No SHAP values available for class index {i}")